{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import concurrent.futures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\trav\\\\trav_dataset1.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, stratify=df['convert_ind'], random_state=24)\n",
    "\n",
    "train_set = train_set.drop(columns=['split'])\n",
    "test_set = test_set.drop(columns=['split'])\n",
    "\n",
    "# Separate features and target from the entire training set\n",
    "y_train = train_set['convert_ind'].values\n",
    "train_x = train_set.drop(columns=['convert_ind'])\n",
    "\n",
    "y_test = test_set['convert_ind'].values\n",
    "test_x = test_set.drop(columns=['convert_ind'])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_x)\n",
    "X_test = scaler.transform(test_x)\n",
    "\n",
    "# Reshape the data for Conv1D (add a channel dimension)\n",
    "X_train = X_train[:, :, np.newaxis]\n",
    "X_test = X_test[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - auc: 0.5436 - loss: 0.9081 - val_auc: 0.6031 - val_loss: 0.7364\n",
      "Epoch 2/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.5620 - loss: 0.7432 - val_auc: 0.6022 - val_loss: 0.6898\n",
      "Epoch 3/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.5801 - loss: 0.7141 - val_auc: 0.6032 - val_loss: 0.6837\n",
      "Epoch 4/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.5887 - loss: 0.6964 - val_auc: 0.6121 - val_loss: 0.6846\n",
      "Epoch 5/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6135 - loss: 0.6791 - val_auc: 0.6305 - val_loss: 0.6895\n",
      "Epoch 6/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6095 - loss: 0.6803 - val_auc: 0.6357 - val_loss: 0.7044\n",
      "Epoch 7/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6243 - loss: 0.6711 - val_auc: 0.6415 - val_loss: 0.7208\n",
      "Epoch 8/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6303 - loss: 0.6782 - val_auc: 0.6388 - val_loss: 0.6967\n",
      "Epoch 9/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - auc: 0.6392 - loss: 0.6555 - val_auc: 0.6357 - val_loss: 0.7176\n",
      "Epoch 10/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - auc: 0.6385 - loss: 0.6651 - val_auc: 0.6383 - val_loss: 0.6953\n",
      "Epoch 11/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6452 - loss: 0.6477 - val_auc: 0.6422 - val_loss: 0.7060\n",
      "Epoch 12/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - auc: 0.6536 - loss: 0.6613 - val_auc: 0.6445 - val_loss: 0.7076\n",
      "Epoch 13/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - auc: 0.6581 - loss: 0.6598 - val_auc: 0.6448 - val_loss: 0.6836\n",
      "Epoch 14/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - auc: 0.6638 - loss: 0.6483 - val_auc: 0.6462 - val_loss: 0.6903\n",
      "Epoch 15/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6578 - loss: 0.6423 - val_auc: 0.6427 - val_loss: 0.7097\n",
      "Epoch 16/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - auc: 0.6636 - loss: 0.6385 - val_auc: 0.6434 - val_loss: 0.6966\n",
      "Epoch 17/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - auc: 0.6594 - loss: 0.6564 - val_auc: 0.6475 - val_loss: 0.6874\n",
      "Epoch 18/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6656 - loss: 0.6465 - val_auc: 0.6460 - val_loss: 0.7044\n",
      "Epoch 19/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - auc: 0.6724 - loss: 0.6483 - val_auc: 0.6478 - val_loss: 0.6844\n",
      "Epoch 20/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6873 - loss: 0.6409 - val_auc: 0.6519 - val_loss: 0.6792\n",
      "Epoch 21/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - auc: 0.6804 - loss: 0.6350 - val_auc: 0.6471 - val_loss: 0.7032\n",
      "Epoch 22/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - auc: 0.6792 - loss: 0.6461 - val_auc: 0.6480 - val_loss: 0.6608\n",
      "Epoch 23/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - auc: 0.6813 - loss: 0.6333 - val_auc: 0.6457 - val_loss: 0.6870\n",
      "Epoch 24/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - auc: 0.6925 - loss: 0.6367 - val_auc: 0.6481 - val_loss: 0.6780\n",
      "Epoch 25/25\n",
      "\u001b[1m1230/1230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - auc: 0.6921 - loss: 0.6328 - val_auc: 0.6462 - val_loss: 0.6912\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# if model is not learning, reduce learning_rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6519\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Test AUC: 0.6517\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "print(f\"Test AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabnet\n",
    "\n",
    "### !pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabnet requires np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, stratify=df['convert_ind'], random_state=24)\n",
    "\n",
    "# Drop the 'split' column if it exists\n",
    "if 'split' in train_set.columns:\n",
    "    train_set = train_set.drop(columns=['split'])\n",
    "    test_set = test_set.drop(columns=['split'])\n",
    "\n",
    "# Separate features and target\n",
    "y_train = train_set['convert_ind'].values\n",
    "train_x = train_set.drop(columns=['convert_ind'])\n",
    "\n",
    "y_test = test_set['convert_ind'].values\n",
    "test_x = test_set.drop(columns=['convert_ind'])\n",
    "\n",
    "# Select numeric columns\n",
    "train_x = train_x.select_dtypes(include=[np.number])\n",
    "test_x = test_x.select_dtypes(include=[np.number])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_x)\n",
    "X_test = scaler.transform(test_x)\n",
    "\n",
    "# Ensure inputs are NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29552 | val_0_auc: 0.56489 |  0:01:22s\n",
      "epoch 1  | loss: 0.28579 | val_0_auc: 0.59315 |  0:02:28s\n",
      "epoch 2  | loss: 0.28362 | val_0_auc: 0.62833 |  0:03:33s\n",
      "epoch 3  | loss: 0.28237 | val_0_auc: 0.61049 |  0:04:37s\n",
      "epoch 4  | loss: 0.2809  | val_0_auc: 0.61878 |  0:05:43s\n",
      "epoch 5  | loss: 0.28116 | val_0_auc: 0.62705 |  0:06:52s\n",
      "epoch 6  | loss: 0.28051 | val_0_auc: 0.62824 |  0:08:05s\n",
      "epoch 7  | loss: 0.28004 | val_0_auc: 0.62607 |  0:09:14s\n",
      "epoch 8  | loss: 0.28055 | val_0_auc: 0.62687 |  0:10:20s\n",
      "epoch 9  | loss: 0.27957 | val_0_auc: 0.63426 |  0:11:25s\n",
      "epoch 10 | loss: 0.27825 | val_0_auc: 0.62437 |  0:12:32s\n",
      "epoch 11 | loss: 0.27945 | val_0_auc: 0.62592 |  0:13:37s\n",
      "epoch 12 | loss: 0.27894 | val_0_auc: 0.62065 |  0:14:42s\n",
      "epoch 13 | loss: 0.27938 | val_0_auc: 0.61556 |  0:15:50s\n",
      "epoch 14 | loss: 0.27887 | val_0_auc: 0.6284  |  0:16:56s\n",
      "epoch 15 | loss: 0.27932 | val_0_auc: 0.6303  |  0:18:01s\n",
      "epoch 16 | loss: 0.27851 | val_0_auc: 0.6318  |  0:19:08s\n",
      "epoch 17 | loss: 0.27817 | val_0_auc: 0.62958 |  0:20:21s\n",
      "epoch 18 | loss: 0.27712 | val_0_auc: 0.63423 |  0:21:35s\n",
      "epoch 19 | loss: 0.2769  | val_0_auc: 0.62991 |  0:22:43s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.63426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.0001, lr=0.01, AUC=0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29355 | val_0_auc: 0.55205 |  0:01:07s\n",
      "epoch 1  | loss: 0.28931 | val_0_auc: 0.59409 |  0:02:14s\n",
      "epoch 2  | loss: 0.28527 | val_0_auc: 0.58824 |  0:03:20s\n",
      "epoch 3  | loss: 0.28487 | val_0_auc: 0.59156 |  0:04:26s\n",
      "epoch 4  | loss: 0.28363 | val_0_auc: 0.60253 |  0:05:33s\n",
      "epoch 5  | loss: 0.28529 | val_0_auc: 0.56909 |  0:06:40s\n",
      "epoch 6  | loss: 0.28451 | val_0_auc: 0.58601 |  0:07:46s\n",
      "epoch 7  | loss: 0.28553 | val_0_auc: 0.59007 |  0:08:53s\n",
      "epoch 8  | loss: 0.28458 | val_0_auc: 0.58996 |  0:10:05s\n",
      "epoch 9  | loss: 0.28625 | val_0_auc: 0.55053 |  0:11:14s\n",
      "epoch 10 | loss: 0.28624 | val_0_auc: 0.56692 |  0:12:21s\n",
      "epoch 11 | loss: 0.28518 | val_0_auc: 0.5722  |  0:13:27s\n",
      "epoch 12 | loss: 0.28438 | val_0_auc: 0.56778 |  0:14:35s\n",
      "epoch 13 | loss: 0.28562 | val_0_auc: 0.58181 |  0:15:42s\n",
      "epoch 14 | loss: 0.28489 | val_0_auc: 0.57716 |  0:16:48s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.60253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.0001, lr=0.05, AUC=0.6025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29469 | val_0_auc: 0.60805 |  0:01:05s\n",
      "epoch 1  | loss: 0.28885 | val_0_auc: 0.58664 |  0:02:11s\n",
      "epoch 2  | loss: 0.28689 | val_0_auc: 0.58488 |  0:03:16s\n",
      "epoch 3  | loss: 0.28717 | val_0_auc: 0.59517 |  0:04:22s\n",
      "epoch 4  | loss: 0.28469 | val_0_auc: 0.59927 |  0:05:33s\n",
      "epoch 5  | loss: 0.28608 | val_0_auc: 0.55749 |  0:06:44s\n",
      "epoch 6  | loss: 0.28684 | val_0_auc: 0.58236 |  0:07:54s\n",
      "epoch 7  | loss: 0.28834 | val_0_auc: 0.59653 |  0:09:04s\n",
      "epoch 8  | loss: 0.28887 | val_0_auc: 0.53977 |  0:10:10s\n",
      "epoch 9  | loss: 0.28764 | val_0_auc: 0.59898 |  0:11:15s\n",
      "epoch 10 | loss: 0.28728 | val_0_auc: 0.56425 |  0:12:21s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.60805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.0001, lr=0.1, AUC=0.6081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29703 | val_0_auc: 0.55135 |  0:01:16s\n",
      "epoch 1  | loss: 0.28768 | val_0_auc: 0.59734 |  0:02:24s\n",
      "epoch 2  | loss: 0.28557 | val_0_auc: 0.60247 |  0:03:31s\n",
      "epoch 3  | loss: 0.28454 | val_0_auc: 0.60034 |  0:04:38s\n",
      "epoch 4  | loss: 0.28272 | val_0_auc: 0.58546 |  0:05:47s\n",
      "epoch 5  | loss: 0.28273 | val_0_auc: 0.62333 |  0:07:05s\n",
      "epoch 6  | loss: 0.28279 | val_0_auc: 0.63212 |  0:08:29s\n",
      "epoch 7  | loss: 0.282   | val_0_auc: 0.63591 |  0:09:44s\n",
      "epoch 8  | loss: 0.28017 | val_0_auc: 0.6488  |  0:10:58s\n",
      "epoch 9  | loss: 0.27896 | val_0_auc: 0.6506  |  0:12:11s\n",
      "epoch 10 | loss: 0.27777 | val_0_auc: 0.65126 |  0:13:23s\n",
      "epoch 11 | loss: 0.27761 | val_0_auc: 0.64785 |  0:14:34s\n",
      "epoch 12 | loss: 0.2776  | val_0_auc: 0.63547 |  0:15:43s\n",
      "epoch 13 | loss: 0.27728 | val_0_auc: 0.64695 |  0:16:52s\n",
      "epoch 14 | loss: 0.2764  | val_0_auc: 0.64777 |  0:17:59s\n",
      "epoch 15 | loss: 0.27732 | val_0_auc: 0.65007 |  0:19:06s\n",
      "epoch 16 | loss: 0.27642 | val_0_auc: 0.65823 |  0:20:14s\n",
      "epoch 17 | loss: 0.27645 | val_0_auc: 0.65217 |  0:21:23s\n",
      "epoch 18 | loss: 0.2763  | val_0_auc: 0.64144 |  0:22:37s\n",
      "epoch 19 | loss: 0.27624 | val_0_auc: 0.6416  |  0:23:55s\n",
      "epoch 20 | loss: 0.27719 | val_0_auc: 0.64652 |  0:25:10s\n",
      "epoch 21 | loss: 0.278   | val_0_auc: 0.64512 |  0:26:24s\n",
      "epoch 22 | loss: 0.27692 | val_0_auc: 0.63944 |  0:27:38s\n",
      "epoch 23 | loss: 0.27642 | val_0_auc: 0.65386 |  0:28:49s\n",
      "epoch 24 | loss: 0.27578 | val_0_auc: 0.65891 |  0:29:59s\n",
      "epoch 25 | loss: 0.276   | val_0_auc: 0.65597 |  0:31:07s\n",
      "epoch 26 | loss: 0.27556 | val_0_auc: 0.6504  |  0:32:14s\n",
      "epoch 27 | loss: 0.2751  | val_0_auc: 0.65932 |  0:33:21s\n",
      "epoch 28 | loss: 0.27489 | val_0_auc: 0.65742 |  0:34:27s\n",
      "epoch 29 | loss: 0.27443 | val_0_auc: 0.65512 |  0:35:34s\n",
      "epoch 30 | loss: 0.27546 | val_0_auc: 0.64883 |  0:36:36s\n",
      "epoch 31 | loss: 0.27647 | val_0_auc: 0.65297 |  0:37:48s\n",
      "epoch 32 | loss: 0.27516 | val_0_auc: 0.65838 |  0:39:07s\n",
      "epoch 33 | loss: 0.27347 | val_0_auc: 0.66605 |  0:40:25s\n",
      "epoch 34 | loss: 0.27357 | val_0_auc: 0.66273 |  0:41:40s\n",
      "epoch 35 | loss: 0.27345 | val_0_auc: 0.66269 |  0:42:52s\n",
      "epoch 36 | loss: 0.27436 | val_0_auc: 0.662   |  0:44:02s\n",
      "epoch 37 | loss: 0.27373 | val_0_auc: 0.66549 |  0:45:12s\n",
      "epoch 38 | loss: 0.2743  | val_0_auc: 0.65287 |  0:46:20s\n",
      "epoch 39 | loss: 0.27323 | val_0_auc: 0.66214 |  0:47:27s\n",
      "epoch 40 | loss: 0.27301 | val_0_auc: 0.66154 |  0:48:34s\n",
      "epoch 41 | loss: 0.27231 | val_0_auc: 0.65975 |  0:49:37s\n",
      "epoch 42 | loss: 0.27154 | val_0_auc: 0.65523 |  0:50:40s\n",
      "epoch 43 | loss: 0.27185 | val_0_auc: 0.6529  |  0:51:44s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_auc = 0.66605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.001, lr=0.01, AUC=0.6660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29502 | val_0_auc: 0.57166 |  0:01:09s\n",
      "epoch 1  | loss: 0.28781 | val_0_auc: 0.57759 |  0:02:19s\n",
      "epoch 2  | loss: 0.28432 | val_0_auc: 0.61575 |  0:03:29s\n",
      "epoch 3  | loss: 0.28441 | val_0_auc: 0.61726 |  0:04:36s\n",
      "epoch 4  | loss: 0.28358 | val_0_auc: 0.60325 |  0:05:41s\n",
      "epoch 5  | loss: 0.28479 | val_0_auc: 0.59616 |  0:06:45s\n",
      "epoch 6  | loss: 0.28365 | val_0_auc: 0.6153  |  0:07:48s\n",
      "epoch 7  | loss: 0.28369 | val_0_auc: 0.61687 |  0:08:51s\n",
      "epoch 8  | loss: 0.28354 | val_0_auc: 0.60776 |  0:09:54s\n",
      "epoch 9  | loss: 0.28173 | val_0_auc: 0.61535 |  0:10:56s\n",
      "epoch 10 | loss: 0.28158 | val_0_auc: 0.63037 |  0:11:58s\n",
      "epoch 11 | loss: 0.28082 | val_0_auc: 0.63043 |  0:12:59s\n",
      "epoch 12 | loss: 0.28063 | val_0_auc: 0.62866 |  0:14:06s\n",
      "epoch 13 | loss: 0.28052 | val_0_auc: 0.63561 |  0:15:16s\n",
      "epoch 14 | loss: 0.28006 | val_0_auc: 0.63492 |  0:16:27s\n",
      "epoch 15 | loss: 0.28015 | val_0_auc: 0.6391  |  0:17:37s\n",
      "epoch 16 | loss: 0.27878 | val_0_auc: 0.64162 |  0:18:44s\n",
      "epoch 17 | loss: 0.2784  | val_0_auc: 0.60773 |  0:19:50s\n",
      "epoch 18 | loss: 0.27775 | val_0_auc: 0.64921 |  0:20:55s\n",
      "epoch 19 | loss: 0.27733 | val_0_auc: 0.63343 |  0:21:59s\n",
      "epoch 20 | loss: 0.2783  | val_0_auc: 0.61951 |  0:23:02s\n",
      "epoch 21 | loss: 0.27803 | val_0_auc: 0.65429 |  0:24:05s\n",
      "epoch 22 | loss: 0.27608 | val_0_auc: 0.64053 |  0:25:07s\n",
      "epoch 23 | loss: 0.27595 | val_0_auc: 0.64259 |  0:26:10s\n",
      "epoch 24 | loss: 0.27463 | val_0_auc: 0.65215 |  0:27:14s\n",
      "epoch 25 | loss: 0.27498 | val_0_auc: 0.65777 |  0:28:21s\n",
      "epoch 26 | loss: 0.2752  | val_0_auc: 0.65424 |  0:29:30s\n",
      "epoch 27 | loss: 0.27595 | val_0_auc: 0.64385 |  0:30:37s\n",
      "epoch 28 | loss: 0.27455 | val_0_auc: 0.64455 |  0:31:41s\n",
      "epoch 29 | loss: 0.27354 | val_0_auc: 0.65758 |  0:32:44s\n",
      "epoch 30 | loss: 0.27365 | val_0_auc: 0.65434 |  0:33:47s\n",
      "epoch 31 | loss: 0.27264 | val_0_auc: 0.64652 |  0:34:49s\n",
      "epoch 32 | loss: 0.27304 | val_0_auc: 0.64746 |  0:35:51s\n",
      "epoch 33 | loss: 0.27209 | val_0_auc: 0.65806 |  0:36:53s\n",
      "epoch 34 | loss: 0.27177 | val_0_auc: 0.65474 |  0:37:54s\n",
      "epoch 35 | loss: 0.27164 | val_0_auc: 0.65914 |  0:38:56s\n",
      "epoch 36 | loss: 0.27117 | val_0_auc: 0.63451 |  0:39:56s\n",
      "epoch 37 | loss: 0.2711  | val_0_auc: 0.65529 |  0:40:57s\n",
      "epoch 38 | loss: 0.27317 | val_0_auc: 0.65953 |  0:42:02s\n",
      "epoch 39 | loss: 0.27125 | val_0_auc: 0.65732 |  0:43:08s\n",
      "epoch 40 | loss: 0.27    | val_0_auc: 0.6586  |  0:44:12s\n",
      "epoch 41 | loss: 0.27051 | val_0_auc: 0.63967 |  0:45:15s\n",
      "epoch 42 | loss: 0.27112 | val_0_auc: 0.64672 |  0:46:18s\n",
      "epoch 43 | loss: 0.27146 | val_0_auc: 0.65658 |  0:47:20s\n",
      "epoch 44 | loss: 0.26913 | val_0_auc: 0.65578 |  0:48:22s\n",
      "epoch 45 | loss: 0.27105 | val_0_auc: 0.66006 |  0:49:26s\n",
      "epoch 46 | loss: 0.2716  | val_0_auc: 0.65719 |  0:50:27s\n",
      "epoch 47 | loss: 0.26931 | val_0_auc: 0.66164 |  0:51:29s\n",
      "epoch 48 | loss: 0.26783 | val_0_auc: 0.65915 |  0:52:30s\n",
      "epoch 49 | loss: 0.26925 | val_0_auc: 0.65831 |  0:53:34s\n",
      "epoch 50 | loss: 0.26903 | val_0_auc: 0.66268 |  0:54:41s\n",
      "epoch 51 | loss: 0.26769 | val_0_auc: 0.65038 |  0:55:48s\n",
      "epoch 52 | loss: 0.26816 | val_0_auc: 0.64919 |  0:56:53s\n",
      "epoch 53 | loss: 0.26791 | val_0_auc: 0.65946 |  0:57:56s\n",
      "epoch 54 | loss: 0.26636 | val_0_auc: 0.64997 |  0:58:59s\n",
      "epoch 55 | loss: 0.2678  | val_0_auc: 0.64658 |  1:00:01s\n",
      "epoch 56 | loss: 0.26824 | val_0_auc: 0.63357 |  1:01:03s\n",
      "epoch 57 | loss: 0.27126 | val_0_auc: 0.65569 |  1:02:04s\n",
      "epoch 58 | loss: 0.26714 | val_0_auc: 0.65414 |  1:03:07s\n",
      "epoch 59 | loss: 0.26733 | val_0_auc: 0.65047 |  1:04:05s\n",
      "epoch 60 | loss: 0.26679 | val_0_auc: 0.64855 |  1:05:05s\n",
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_auc = 0.66268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.001, lr=0.05, AUC=0.6627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29438 | val_0_auc: 0.5692  |  0:01:06s\n",
      "epoch 1  | loss: 0.28833 | val_0_auc: 0.58183 |  0:02:16s\n",
      "epoch 2  | loss: 0.28479 | val_0_auc: 0.58308 |  0:03:28s\n",
      "epoch 3  | loss: 0.28619 | val_0_auc: 0.58552 |  0:04:37s\n",
      "epoch 4  | loss: 0.2865  | val_0_auc: 0.54898 |  0:05:45s\n",
      "epoch 5  | loss: 0.288   | val_0_auc: 0.60191 |  0:06:53s\n",
      "epoch 6  | loss: 0.28556 | val_0_auc: 0.59052 |  0:08:00s\n",
      "epoch 7  | loss: 0.28721 | val_0_auc: 0.5763  |  0:09:07s\n",
      "epoch 8  | loss: 0.28771 | val_0_auc: 0.5598  |  0:10:14s\n",
      "epoch 9  | loss: 0.28573 | val_0_auc: 0.58788 |  0:11:19s\n",
      "epoch 10 | loss: 0.28483 | val_0_auc: 0.59195 |  0:12:32s\n",
      "epoch 11 | loss: 0.2849  | val_0_auc: 0.56426 |  0:13:37s\n",
      "epoch 12 | loss: 0.28495 | val_0_auc: 0.5846  |  0:14:41s\n",
      "epoch 13 | loss: 0.28643 | val_0_auc: 0.57364 |  0:15:47s\n",
      "epoch 14 | loss: 0.28696 | val_0_auc: 0.56312 |  0:16:55s\n",
      "epoch 15 | loss: 0.28664 | val_0_auc: 0.60726 |  0:18:02s\n",
      "epoch 16 | loss: 0.28549 | val_0_auc: 0.60512 |  0:19:08s\n",
      "epoch 17 | loss: 0.28583 | val_0_auc: 0.59788 |  0:20:17s\n",
      "epoch 18 | loss: 0.28423 | val_0_auc: 0.59444 |  0:21:38s\n",
      "epoch 19 | loss: 0.28481 | val_0_auc: 0.6096  |  0:22:55s\n",
      "epoch 20 | loss: 0.28385 | val_0_auc: 0.61033 |  0:24:01s\n",
      "epoch 21 | loss: 0.28316 | val_0_auc: 0.6024  |  0:25:11s\n",
      "epoch 22 | loss: 0.28567 | val_0_auc: 0.59909 |  0:26:13s\n",
      "epoch 23 | loss: 0.28334 | val_0_auc: 0.60411 |  0:27:09s\n",
      "epoch 24 | loss: 0.28379 | val_0_auc: 0.61182 |  0:28:10s\n",
      "epoch 25 | loss: 0.2831  | val_0_auc: 0.60939 |  0:29:12s\n",
      "epoch 26 | loss: 0.28496 | val_0_auc: 0.60953 |  0:30:15s\n",
      "epoch 27 | loss: 0.28311 | val_0_auc: 0.59915 |  0:31:22s\n",
      "epoch 28 | loss: 0.28357 | val_0_auc: 0.60404 |  0:32:31s\n",
      "epoch 29 | loss: 0.283   | val_0_auc: 0.60968 |  0:33:41s\n",
      "epoch 30 | loss: 0.28337 | val_0_auc: 0.61026 |  0:34:51s\n",
      "epoch 31 | loss: 0.2826  | val_0_auc: 0.60774 |  0:36:00s\n",
      "epoch 32 | loss: 0.28292 | val_0_auc: 0.59954 |  0:37:13s\n",
      "epoch 33 | loss: 0.28407 | val_0_auc: 0.60476 |  0:38:28s\n",
      "epoch 34 | loss: 0.28397 | val_0_auc: 0.60345 |  0:39:47s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.61182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.001, lr=0.1, AUC=0.6118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.30448 | val_0_auc: 0.58475 |  0:01:09s\n",
      "epoch 1  | loss: 0.29166 | val_0_auc: 0.58747 |  0:02:17s\n",
      "epoch 2  | loss: 0.28882 | val_0_auc: 0.58621 |  0:03:26s\n",
      "epoch 3  | loss: 0.28684 | val_0_auc: 0.60022 |  0:04:32s\n",
      "epoch 4  | loss: 0.28602 | val_0_auc: 0.60779 |  0:05:37s\n",
      "epoch 5  | loss: 0.28538 | val_0_auc: 0.60367 |  0:06:44s\n",
      "epoch 6  | loss: 0.28434 | val_0_auc: 0.59923 |  0:07:53s\n",
      "epoch 7  | loss: 0.28467 | val_0_auc: 0.6239  |  0:09:03s\n",
      "epoch 8  | loss: 0.28347 | val_0_auc: 0.61881 |  0:10:14s\n",
      "epoch 9  | loss: 0.28343 | val_0_auc: 0.63149 |  0:11:23s\n",
      "epoch 10 | loss: 0.28192 | val_0_auc: 0.63633 |  0:12:31s\n",
      "epoch 11 | loss: 0.28168 | val_0_auc: 0.63498 |  0:13:38s\n",
      "epoch 12 | loss: 0.28092 | val_0_auc: 0.63314 |  0:14:44s\n",
      "epoch 13 | loss: 0.28099 | val_0_auc: 0.63791 |  0:15:48s\n",
      "epoch 14 | loss: 0.27935 | val_0_auc: 0.63942 |  0:16:51s\n",
      "epoch 15 | loss: 0.27991 | val_0_auc: 0.63987 |  0:17:54s\n",
      "epoch 16 | loss: 0.27942 | val_0_auc: 0.63826 |  0:18:56s\n",
      "epoch 17 | loss: 0.27933 | val_0_auc: 0.64141 |  0:20:00s\n",
      "epoch 18 | loss: 0.27885 | val_0_auc: 0.6356  |  0:21:03s\n",
      "epoch 19 | loss: 0.2787  | val_0_auc: 0.635   |  0:22:09s\n",
      "epoch 20 | loss: 0.27844 | val_0_auc: 0.63728 |  0:23:17s\n",
      "epoch 21 | loss: 0.28031 | val_0_auc: 0.63939 |  0:24:27s\n",
      "epoch 22 | loss: 0.28075 | val_0_auc: 0.61955 |  0:25:36s\n",
      "epoch 23 | loss: 0.28067 | val_0_auc: 0.63495 |  0:26:47s\n",
      "epoch 24 | loss: 0.27947 | val_0_auc: 0.63024 |  0:27:55s\n",
      "epoch 25 | loss: 0.28069 | val_0_auc: 0.63144 |  0:29:01s\n",
      "epoch 26 | loss: 0.27977 | val_0_auc: 0.62454 |  0:30:05s\n",
      "epoch 27 | loss: 0.28    | val_0_auc: 0.6237  |  0:31:08s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.64141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.01, lr=0.01, AUC=0.6414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.2953  | val_0_auc: 0.60596 |  0:01:01s\n",
      "epoch 1  | loss: 0.28949 | val_0_auc: 0.6225  |  0:02:03s\n",
      "epoch 2  | loss: 0.28587 | val_0_auc: 0.62102 |  0:03:05s\n",
      "epoch 3  | loss: 0.28541 | val_0_auc: 0.62269 |  0:04:08s\n",
      "epoch 4  | loss: 0.28382 | val_0_auc: 0.62249 |  0:05:11s\n",
      "epoch 5  | loss: 0.28475 | val_0_auc: 0.61103 |  0:06:17s\n",
      "epoch 6  | loss: 0.28411 | val_0_auc: 0.60868 |  0:07:24s\n",
      "epoch 7  | loss: 0.28426 | val_0_auc: 0.61167 |  0:08:30s\n",
      "epoch 8  | loss: 0.28382 | val_0_auc: 0.61783 |  0:09:34s\n",
      "epoch 9  | loss: 0.28297 | val_0_auc: 0.61351 |  0:10:38s\n",
      "epoch 10 | loss: 0.28288 | val_0_auc: 0.60988 |  0:11:33s\n",
      "epoch 11 | loss: 0.28248 | val_0_auc: 0.61035 |  0:12:30s\n",
      "epoch 12 | loss: 0.28286 | val_0_auc: 0.61745 |  0:13:30s\n",
      "epoch 13 | loss: 0.28292 | val_0_auc: 0.62081 |  0:14:30s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.62269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.01, lr=0.05, AUC=0.6227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29866 | val_0_auc: 0.54842 |  0:00:59s\n",
      "epoch 1  | loss: 0.29112 | val_0_auc: 0.58499 |  0:02:00s\n",
      "epoch 2  | loss: 0.28821 | val_0_auc: 0.58909 |  0:03:02s\n",
      "epoch 3  | loss: 0.28775 | val_0_auc: 0.57568 |  0:04:05s\n",
      "epoch 4  | loss: 0.28675 | val_0_auc: 0.56711 |  0:05:10s\n",
      "epoch 5  | loss: 0.28768 | val_0_auc: 0.57178 |  0:06:16s\n",
      "epoch 6  | loss: 0.28631 | val_0_auc: 0.56516 |  0:07:23s\n",
      "epoch 7  | loss: 0.28783 | val_0_auc: 0.55862 |  0:08:28s\n",
      "epoch 8  | loss: 0.28854 | val_0_auc: 0.5743  |  0:09:31s\n",
      "epoch 9  | loss: 0.28707 | val_0_auc: 0.57363 |  0:10:33s\n",
      "epoch 10 | loss: 0.28647 | val_0_auc: 0.56731 |  0:11:35s\n",
      "epoch 11 | loss: 0.28616 | val_0_auc: 0.58201 |  0:12:37s\n",
      "epoch 12 | loss: 0.28718 | val_0_auc: 0.51469 |  0:13:38s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.58909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.0, lambda_sparse=0.01, lr=0.1, AUC=0.5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29764 | val_0_auc: 0.55405 |  0:01:00s\n",
      "epoch 1  | loss: 0.28765 | val_0_auc: 0.56507 |  0:02:01s\n",
      "epoch 2  | loss: 0.2866  | val_0_auc: 0.57512 |  0:03:09s\n",
      "epoch 3  | loss: 0.28515 | val_0_auc: 0.57504 |  0:04:15s\n",
      "epoch 4  | loss: 0.28496 | val_0_auc: 0.58999 |  0:05:22s\n",
      "epoch 5  | loss: 0.28423 | val_0_auc: 0.60385 |  0:06:29s\n",
      "epoch 6  | loss: 0.28339 | val_0_auc: 0.60589 |  0:07:36s\n",
      "epoch 7  | loss: 0.28323 | val_0_auc: 0.60401 |  0:08:40s\n",
      "epoch 8  | loss: 0.28282 | val_0_auc: 0.59753 |  0:09:47s\n",
      "epoch 9  | loss: 0.28328 | val_0_auc: 0.61164 |  0:10:49s\n",
      "epoch 10 | loss: 0.28191 | val_0_auc: 0.60932 |  0:11:51s\n",
      "epoch 11 | loss: 0.2807  | val_0_auc: 0.61666 |  0:12:54s\n",
      "epoch 12 | loss: 0.28098 | val_0_auc: 0.61198 |  0:13:56s\n",
      "epoch 13 | loss: 0.28119 | val_0_auc: 0.6279  |  0:14:59s\n",
      "epoch 14 | loss: 0.27965 | val_0_auc: 0.6265  |  0:16:02s\n",
      "epoch 15 | loss: 0.27958 | val_0_auc: 0.63474 |  0:17:05s\n",
      "epoch 16 | loss: 0.27967 | val_0_auc: 0.62274 |  0:18:10s\n",
      "epoch 17 | loss: 0.27928 | val_0_auc: 0.62724 |  0:19:16s\n",
      "epoch 18 | loss: 0.27835 | val_0_auc: 0.63974 |  0:20:23s\n",
      "epoch 19 | loss: 0.27757 | val_0_auc: 0.6381  |  0:21:30s\n",
      "epoch 20 | loss: 0.27761 | val_0_auc: 0.63675 |  0:22:34s\n",
      "epoch 21 | loss: 0.2781  | val_0_auc: 0.6459  |  0:23:37s\n",
      "epoch 22 | loss: 0.27601 | val_0_auc: 0.64401 |  0:24:41s\n",
      "epoch 23 | loss: 0.27636 | val_0_auc: 0.6422  |  0:25:43s\n",
      "epoch 24 | loss: 0.2761  | val_0_auc: 0.64938 |  0:26:45s\n",
      "epoch 25 | loss: 0.27591 | val_0_auc: 0.63782 |  0:27:50s\n",
      "epoch 26 | loss: 0.27594 | val_0_auc: 0.65068 |  0:28:52s\n",
      "epoch 27 | loss: 0.27578 | val_0_auc: 0.65201 |  0:29:55s\n",
      "epoch 28 | loss: 0.27519 | val_0_auc: 0.65353 |  0:30:58s\n",
      "epoch 29 | loss: 0.27453 | val_0_auc: 0.65357 |  0:32:03s\n",
      "epoch 30 | loss: 0.275   | val_0_auc: 0.65614 |  0:33:09s\n",
      "epoch 31 | loss: 0.27341 | val_0_auc: 0.66125 |  0:34:15s\n",
      "epoch 32 | loss: 0.27443 | val_0_auc: 0.64963 |  0:35:18s\n",
      "epoch 33 | loss: 0.27436 | val_0_auc: 0.65699 |  0:36:20s\n",
      "epoch 34 | loss: 0.27438 | val_0_auc: 0.66144 |  0:37:23s\n",
      "epoch 35 | loss: 0.27381 | val_0_auc: 0.66125 |  0:38:25s\n",
      "epoch 36 | loss: 0.27364 | val_0_auc: 0.65222 |  0:39:25s\n",
      "epoch 37 | loss: 0.27297 | val_0_auc: 0.65554 |  0:40:26s\n",
      "epoch 38 | loss: 0.27384 | val_0_auc: 0.6584  |  0:41:28s\n",
      "epoch 39 | loss: 0.27375 | val_0_auc: 0.65674 |  0:42:31s\n",
      "epoch 40 | loss: 0.2726  | val_0_auc: 0.65634 |  0:43:33s\n",
      "epoch 41 | loss: 0.27189 | val_0_auc: 0.65516 |  0:44:37s\n",
      "epoch 42 | loss: 0.2721  | val_0_auc: 0.65559 |  0:45:41s\n",
      "epoch 43 | loss: 0.27181 | val_0_auc: 0.65683 |  0:46:47s\n",
      "epoch 44 | loss: 0.27193 | val_0_auc: 0.65491 |  0:47:52s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_auc = 0.66144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.5, lambda_sparse=0.0001, lr=0.01, AUC=0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.2925  | val_0_auc: 0.60837 |  0:01:04s\n",
      "epoch 1  | loss: 0.28663 | val_0_auc: 0.60076 |  0:02:07s\n",
      "epoch 2  | loss: 0.28391 | val_0_auc: 0.60166 |  0:03:12s\n",
      "epoch 3  | loss: 0.28468 | val_0_auc: 0.61228 |  0:04:13s\n",
      "epoch 4  | loss: 0.28361 | val_0_auc: 0.60704 |  0:05:18s\n",
      "epoch 5  | loss: 0.28384 | val_0_auc: 0.62126 |  0:06:19s\n",
      "epoch 6  | loss: 0.283   | val_0_auc: 0.6279  |  0:07:18s\n",
      "epoch 7  | loss: 0.28327 | val_0_auc: 0.63156 |  0:08:24s\n",
      "epoch 8  | loss: 0.28312 | val_0_auc: 0.63224 |  0:09:24s\n",
      "epoch 9  | loss: 0.28184 | val_0_auc: 0.62674 |  0:10:24s\n",
      "epoch 10 | loss: 0.28121 | val_0_auc: 0.61855 |  0:11:26s\n",
      "epoch 11 | loss: 0.28003 | val_0_auc: 0.62582 |  0:12:27s\n",
      "epoch 12 | loss: 0.28007 | val_0_auc: 0.6231  |  0:13:27s\n",
      "epoch 13 | loss: 0.28053 | val_0_auc: 0.6438  |  0:14:42s\n",
      "epoch 14 | loss: 0.27925 | val_0_auc: 0.64049 |  0:15:49s\n",
      "epoch 15 | loss: 0.28027 | val_0_auc: 0.63901 |  0:16:57s\n",
      "epoch 16 | loss: 0.27948 | val_0_auc: 0.64428 |  0:18:05s\n",
      "epoch 17 | loss: 0.27921 | val_0_auc: 0.64479 |  0:19:14s\n",
      "epoch 18 | loss: 0.27874 | val_0_auc: 0.64912 |  0:20:22s\n",
      "epoch 19 | loss: 0.27829 | val_0_auc: 0.64555 |  0:21:31s\n",
      "epoch 20 | loss: 0.27775 | val_0_auc: 0.64348 |  0:22:44s\n",
      "epoch 21 | loss: 0.27823 | val_0_auc: 0.64965 |  0:23:48s\n",
      "epoch 22 | loss: 0.27782 | val_0_auc: 0.6236  |  0:24:56s\n",
      "epoch 23 | loss: 0.2779  | val_0_auc: 0.64784 |  0:26:03s\n",
      "epoch 24 | loss: 0.27728 | val_0_auc: 0.64908 |  0:27:18s\n",
      "epoch 25 | loss: 0.27705 | val_0_auc: 0.65956 |  0:28:35s\n",
      "epoch 26 | loss: 0.27675 | val_0_auc: 0.64283 |  0:29:48s\n",
      "epoch 27 | loss: 0.27642 | val_0_auc: 0.66294 |  0:31:00s\n",
      "epoch 28 | loss: 0.27583 | val_0_auc: 0.64941 |  0:32:08s\n",
      "epoch 29 | loss: 0.2755  | val_0_auc: 0.64776 |  0:33:21s\n",
      "epoch 30 | loss: 0.28026 | val_0_auc: 0.60513 |  0:34:39s\n",
      "epoch 31 | loss: 0.28236 | val_0_auc: 0.63084 |  0:35:49s\n",
      "epoch 32 | loss: 0.28287 | val_0_auc: 0.62345 |  0:36:58s\n",
      "epoch 33 | loss: 0.28242 | val_0_auc: 0.62655 |  0:38:04s\n",
      "epoch 34 | loss: 0.28279 | val_0_auc: 0.61385 |  0:39:09s\n",
      "epoch 35 | loss: 0.28262 | val_0_auc: 0.63309 |  0:40:13s\n",
      "epoch 36 | loss: 0.28259 | val_0_auc: 0.62617 |  0:41:17s\n",
      "epoch 37 | loss: 0.28225 | val_0_auc: 0.63251 |  0:42:34s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.66294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: n_d=8, n_a=8, n_steps=3, gamma=1.5, lambda_sparse=0.0001, lr=0.05, AUC=0.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29491 | val_0_auc: 0.58021 |  0:01:04s\n",
      "epoch 1  | loss: 0.29021 | val_0_auc: 0.55617 |  0:02:07s\n",
      "epoch 2  | loss: 0.29039 | val_0_auc: 0.55875 |  0:03:16s\n",
      "epoch 3  | loss: 0.28932 | val_0_auc: 0.58041 |  0:04:29s\n",
      "epoch 4  | loss: 0.28831 | val_0_auc: 0.61111 |  0:05:31s\n",
      "epoch 5  | loss: 0.28703 | val_0_auc: 0.60974 |  0:06:33s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "best_auc = 0\n",
    "best_params = {}\n",
    "\n",
    "# Parameter ranges\n",
    "param_grid = {\n",
    "    'n_d': [8, 16, 32],\n",
    "    'n_a': [8, 16, 32],\n",
    "    'n_steps': [3, 5, 7],\n",
    "    'gamma': [1.0, 1.5, 2.0],\n",
    "    'lambda_sparse': [1e-4, 1e-3, 1e-2],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "}\n",
    "\n",
    "# Manual loop for tuning\n",
    "for n_d in param_grid['n_d']:\n",
    "    for n_a in param_grid['n_a']:\n",
    "        for n_steps in param_grid['n_steps']:\n",
    "            for gamma in param_grid['gamma']:\n",
    "                for lambda_sparse in param_grid['lambda_sparse']:\n",
    "                    for lr in param_grid['learning_rate']:\n",
    "                        # Define the model\n",
    "                        model = TabNetClassifier(\n",
    "                            n_d=n_d,\n",
    "                            n_a=n_a,\n",
    "                            n_steps=n_steps,\n",
    "                            gamma=gamma,\n",
    "                            lambda_sparse=lambda_sparse,\n",
    "                            optimizer_params=dict(lr=lr),\n",
    "                        )\n",
    "                        \n",
    "                        # Train the model\n",
    "                        model.fit(\n",
    "                            X_train, y_train,\n",
    "                            eval_set=[(X_test, y_test)],\n",
    "                            eval_metric=['auc'],\n",
    "                            max_epochs=100,\n",
    "                            batch_size=128,\n",
    "                            patience=10\n",
    "                        )\n",
    "                        \n",
    "                        # Evaluate the model\n",
    "                        preds = model.predict_proba(X_test)[:, 1]\n",
    "                        auc = roc_auc_score(y_test, preds)\n",
    "                        print(f\"Params: n_d={n_d}, n_a={n_a}, n_steps={n_steps}, gamma={gamma}, lambda_sparse={lambda_sparse}, lr={lr}, AUC={auc:.4f}\")\n",
    "\n",
    "                        # Track the best parameters\n",
    "                        if auc > best_auc:\n",
    "                            best_auc = auc\n",
    "                            best_params = {\n",
    "                                'n_d': n_d,\n",
    "                                'n_a': n_a,\n",
    "                                'n_steps': n_steps,\n",
    "                                'gamma': gamma,\n",
    "                                'lambda_sparse': lambda_sparse,\n",
    "                                'learning_rate': lr,\n",
    "                            }\n",
    "\n",
    "print(\"Best AUC:\", best_auc)\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "print(f\"Test AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jl2815",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
