{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\trav\\\\vehicles.csv\")\n",
    "policies = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\trav\\\\policies.csv\")\n",
    "drivers = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\trav\\\\drivers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "For categorical variables, I plan to use mode to fill missings but for continuous variables, I want to see the actual distributions of variables to choose between median and mean to fill missings. \n",
    "\n",
    "For skewed data, median can better represent central tendency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_encode(df:pd.DataFrame, col_name:str):\n",
    "    tmp = df[col_name].mode()\n",
    "    df[col_name] = df[col_name].fillna(tmp[0])\n",
    "    \n",
    "def mean_encode(df:pd.DataFrame, col_name:str):\n",
    "    tmp = df[col_name].mean()\n",
    "    df[col_name] = df[col_name].fillna(tmp)\n",
    "\n",
    "def median_encode(df:pd.DataFrame, col_name:str):\n",
    "    tmp = df[col_name].median()\n",
    "    df[col_name] = df[col_name].fillna(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show some skewed variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of quoted_amt: 3.619782946241044\n",
      "Skewness of vehicle age: 0.45082232812486467\n",
      "Skewness of driver age: 0.2728148452253024\n",
      "Skewness of drivers safty rating: -0.9619020823577018\n"
     ]
    }
   ],
   "source": [
    "# quoted_amt has a form of \"$4,500\". Make it float.\n",
    "policies['quoted_amt'] = policies['quoted_amt'].str[1:]\n",
    "policies['quoted_amt'] = policies['quoted_amt'].astype(str).str.replace(',','').astype(float)\n",
    "policies['state_safty'] =  policies['state_id'].apply( lambda x: 1 if x in ['CT','MN' ,'WI'] else (3 if x in ['NY','AL','GA','NJ'] else 2))\n",
    "\n",
    "print(f'Skewness of quoted_amt: {policies['quoted_amt'].skew()}')\n",
    "print(f'Skewness of vehicle age: {vehicles['age'].skew() }')\n",
    "print(f'Skewness of driver age: {drivers['age'].skew() }')\n",
    "print(f'Skewness of drivers safty rating: {drivers['safty_rating'].skew() }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliars, Scaling, skewness\n",
    "\n",
    "Without much feature engineering, I compared basic CNN, LightGBM, and other models to evaluate their performance. I decided to use LightGBM, as it seemed the most promising based on its results.\n",
    "\n",
    "Since scaling, outliers, and the skewness of variables are less critical for tree-based models, I chose to focus on other aspects instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Caveat: DO NOT CREATE NEW VARIABLES BEFORE MODELING ON THE GIVEN DATASET.\n",
    "Reason: If a newly created variable is noisy and becomes influential in a tree model, it can be misleading and mask other important features.\n",
    "\n",
    "Diagnostic: When optimizing hyperparameters, if the optimized max_depth is too small (like 4 or 5) relative to the number of features in the dataset, you want to be careful because the model may stop growing deeper trees because it doesn't find additional splits helpful and if the splits rely on wrong variable, the model can be misleading.  Removing such misleading factors can significantly improve the model's performance and allow other variables to become influential again.\n",
    "\n",
    "Example: I created a new feature, make_risk, which served as an indicator for risky car brands. This feature was quite subjective and turned out to be misleading. After removing it, the model's performance dramatically improved, and the discount variable, whose influence was previously masked, became important again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies['state_safty'] =  policies['state_id'].apply( lambda x: 1 if x in ['CT','MN' ,'WI'] else (3 if x in ['NY','AL','GA','NJ'] else 2))\n",
    "policies['discount'] = policies['discount'].apply(lambda x: 1 if x=='Yes' else 0 )\n",
    "policies['Home_policy_ind'] = policies['Home_policy_ind'].apply(lambda x: 1 if x=='Y' else 0 )\n",
    "policies['Cov_package_type'] = policies['Cov_package_type'].apply(lambda x: 1 if x=='Low' else (2 if x=='Medium' else 3) )\n",
    "policies['primary_parking'] = policies['primary_parking'].apply(lambda x: 1 if x=='street' else (3 if x=='parking garage' or x=='home/driveway' else 2) )\n",
    "\n",
    "policies['quote_year'] = pd.to_datetime(policies['Quote_dt']).dt.year\n",
    "policies['quote_month'] = pd.to_datetime(policies['Quote_dt']).dt.month\n",
    "policies = policies.drop(columns=['Quote_dt','Unnamed: 0'])\n",
    "\n",
    "drivers.rename(columns={'age':'dr_age'}, inplace=True)\n",
    "drivers=pd.get_dummies(drivers,columns=[\"gender\"])\n",
    "living_status_dict = {'dependent': 1,'rent': 2,'own': 3}  # scaling less impact on tree model\n",
    "drivers['living_status']= drivers['living_status'].map(living_status_dict)\n",
    "drivers['dr_age'] = np.sqrt(drivers['dr_age']  )\n",
    "drivers1 = drivers.groupby('policy_id')[['dr_age','safty_rating']].mean()\n",
    "# For car insurance, it does not make sense to aggreate driver's age over each household or policy_id because \n",
    "# when I looked at the age, there were significant number of people with age over 80 90 or even 100, and I don't think they will drive \n",
    "# by themselves, if this was a life insurance it is matter but this is a car insurance \n",
    "# I may transform age data by square root transformation and this will compress the influence of older ages\n",
    "drivers = drivers.drop(columns=['Unnamed: 0'])\n",
    "drivers = drivers.groupby('policy_id').mean()\n",
    "\n",
    "owner_dict = {'leased': 1,'loaned': 2,'owned': 3} \n",
    "vehicles['ownership_type']= vehicles['ownership_type'].map(owner_dict)\n",
    "vehicles = vehicles.drop(columns=['Unnamed: 0','color' ,'make_model'])\n",
    "vehicles.rename(columns={'age':'car_age'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_encode(vehicles, 'car_age')\n",
    "mode_encode(drivers, 'safty_rating')\n",
    "mean_encode(policies,'quoted_amt') # no median \n",
    "mode_encode(policies,'zip')\n",
    "mean_encode(policies,'credit_score')\n",
    "mode_encode(policies,'Prior_carrier_grp')\n",
    "mode_encode(policies,'Cov_package_type')\n",
    "mode_encode(policies,'CAT_zone')\n",
    "mode_encode(policies,'convert_ind')\n",
    "mode_encode(policies,'Agent_cd')\n",
    "mode_encode(drivers,'high_education_ind')\n",
    "mode_encode(drivers,'living_status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge three data sets: drivers, policies, policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(policies, vehicles, on = 'policy_id', how='inner')\n",
    "df = pd.merge(df1, drivers, on = 'policy_id', how='inner')\n",
    "df = df[df['split']=='Train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cardinality Categorical Variables\n",
    "\n",
    "Variables like 'zip', 'county_name', 'policy_id' have high cardinality and simple one-hot encoding may not be helpful. \n",
    "\n",
    "Aggregation: The process of summarizing data by applying a function (sum, mean, count) to group-level subsets. \n",
    "\n",
    "We can simply remove them or REPLACE WITH AN AGGREGATED TARGET VALUE (e.g., average conversion rate for each county).\n",
    "\n",
    "It turns out that conversion rates for each county, prior insurer, and catastrophe level are influential features in a tree-based model.\n",
    "\n",
    "county_conversion_rate = df.groupby('county_name', observed=True)['convert_ind'].mean()\n",
    "df['county_conversion_rate'] = df['county_name'].map(county_conversion_rate)\n",
    "\n",
    "cat_conversion_rate = df.groupby('CAT_zone', observed=True)['convert_ind'].mean()\n",
    "df['CAT_zone_conversion_rate'] = df['CAT_zone'].map(cat_conversion_rate)\n",
    "\n",
    "Prior_conversion_rate = df.groupby('Prior_carrier_grp', observed=True)['convert_ind'].mean()\n",
    "df['Prior_conversion_rate'] = df['Prior_carrier_grp'].map(Prior_conversion_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount: 2\n",
      "Home_policy_ind: 2\n",
      "zip: 1168\n",
      "state_id: 8\n",
      "county_name: 112\n",
      "Agent_cd: 1628\n",
      "quoted_amt: 12622\n",
      "Prior_carrier_grp: 9\n",
      "credit_score: 418\n",
      "Cov_package_type: 3\n",
      "CAT_zone: 5\n",
      "policy_id: 36871\n",
      "number_drivers: 6\n",
      "num_loaned_veh: 4\n",
      "num_owned_veh: 3\n",
      "num_leased_veh: 3\n",
      "total_number_veh: 8\n",
      "convert_ind: 2\n",
      "split: 1\n",
      "primary_parking: 3\n",
      "state_safty: 3\n",
      "quote_year: 4\n",
      "quote_month: 12\n",
      "car_no: 8\n",
      "ownership_type: 3\n",
      "car_age: 17\n",
      "living_status: 12\n",
      "dr_age: 9640\n",
      "safty_rating: 581\n",
      "high_education_ind: 11\n",
      "gender_F: 13\n",
      "gender_M: 13\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(f'{x}: {df[x].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for other feature engineering\n",
    "\n",
    "Sometimes you have to go back and forth to create better features because model output themselves can give you an idea. \n",
    "\n",
    "Below are some ideas worth trying:\n",
    "\n",
    "1. Discretize continuous variables such as credit_score. \n",
    "\n",
    "df['credit_score_bin'] = pd.cut(df['credit_score'], bins=[0, 600, 700, 800, 900], labels=['low', 'medium', 'high', 'excellent']).astype('category')\n",
    "\n",
    "2. Try different combination of interaction terms.\n",
    "\n",
    "df['driver_vehicle_ratio'] = df['number_drivers'] / df['total_number_veh']   \n",
    "df['quoted_amt_discount'] = (df['discount']+1)* df['quoted_amt']\n",
    "\n",
    "3. How long has it passed since quoted data.   \n",
    "\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['quote_month'] / 12)   \n",
    "df['month_cos'] = np.cos(2 * np.pi * df['quote_month'] / 12)   \n",
    "df['time_since_quote'] = 2024 - df['quote_year'] + (12 - df    ['quote_month']) / 12    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(policies, vehicles, on = 'policy_id', how='inner')\n",
    "df = pd.merge(df1, drivers, on = 'policy_id', how='inner')\n",
    "df = df[df['split']=='Train']\n",
    "df = df.drop(columns=['policy_id','split', 'zip'])\n",
    "\n",
    "categorical_columns = ['state_id', 'county_name', 'Prior_carrier_grp']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# df['total_vehicles'] = df['num_owned_veh'] + df['num_leased_veh'] + df['num_loaned_veh']\n",
    "\n",
    "df = df.drop(columns=['num_owned_veh','num_leased_veh','num_loaned_veh'])\n",
    "\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['quote_month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['quote_month'] / 12)\n",
    "# df= df.drop(columns=['quote_month'])  0.0002 we need this\n",
    "df['quote_year_cat'] = df['quote_year'].astype('category')\n",
    "\n",
    "df['credit_score_bin'] = pd.cut(df['credit_score'], bins=[0, 600, 700, 800, 900], labels=['low', 'medium', 'high', 'excellent']).astype('category')\n",
    "\n",
    "df = df.drop(columns=['credit_score'])  # better than 0 600 800 900 or 0 500 600 700 800 900\n",
    "\n",
    "df['driver_vehicle_ratio'] = df['number_drivers'] / df['total_number_veh']\n",
    "\n",
    "df = df.drop(columns=['state_id'])\n",
    "\n",
    "county_conversion_rate = df.groupby('county_name', observed=True)['convert_ind'].mean()\n",
    "df['county_conversion_rate'] = df['county_name'].map(county_conversion_rate)\n",
    "\n",
    "cat_conversion_rate = df.groupby('CAT_zone', observed=True)['convert_ind'].mean()\n",
    "df['CAT_zone_conversion_rate'] = df['CAT_zone'].map(cat_conversion_rate)\n",
    "\n",
    "Prior_conversion_rate = df.groupby('Prior_carrier_grp', observed=True)['convert_ind'].mean()\n",
    "df['Prior_conversion_rate'] = df['Prior_carrier_grp'].map(Prior_conversion_rate)\n",
    "\n",
    "# df= df.drop(columns=['state_safty']) this is helpful\n",
    "\n",
    "df['time_since_quote'] = 2024 - df['quote_year'] + (12 - df['quote_month']) / 12\n",
    "\n",
    "df= df.drop(columns=['Agent_cd','county_name']) # 0.6947 to 0.7041 \n",
    "\n",
    "df['quoted_amt_discount'] = (df['discount']+1)* df['quoted_amt']\n",
    "\n",
    "df.to_csv(\"C:\\\\Users\\\\joonw\\\\trav\\\\trav_dataset3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not helpful feature engineering:  \n",
    "\n",
    "df['has_discount'] = (df['discount'] > 0).astype(int)\n",
    "\n",
    "agent_freq = df['Agent_cd'].value_counts()\n",
    "df['agent_freq'] = df['Agent_cd'].map(agent_freq)\n",
    "\n",
    "df['log_quoted_amt'] = np.log1p(df['quoted_amt'])\n",
    "\n",
    "First two digits of policy_id or zip code\n",
    "\n",
    "df['policy_id_1'] = df['policy_id'].astype(str).str[8:8+1].astype('category')\n",
    "df['zip_1'] = df['zip'].astype(str).str[:1].astype('category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jl2815",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
